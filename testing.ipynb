{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from six import iteritems\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/fuzzywuzzy/fuzz.py:35: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from Litho.nlp_funcs import *\n",
    "from Litho.similarity import (check_similarity, match_lithcode, jaccard_similarity, \n",
    "                              calc_similarity_score, print_sim_compare, merge_similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.getcwd()+'/'\n",
    "file = 'reclasified.csv'\n",
    "\n",
    "DF = pd.read_csv(root+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopa = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130085,) (130085,)\n"
     ]
    }
   ],
   "source": [
    "DF = DF[['Description']]\n",
    "DF = DF.dropna(how = 'any')\n",
    "Descriptions=DF.Description.tolist()\n",
    "Descriptions = [str(n) for n in Descriptions]\n",
    "\n",
    "#use extend so it's a big flat list of vocab\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in Descriptions:\n",
    "    allwords_stemmed = tokenize_and_stem(i, stop) #for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_stemmed.extend([allwords_stemmed]) #extend the 'totalvocab_stemmed' list\n",
    "\n",
    "    allwords_tokenized = tokenize_only(i, stop)\n",
    "    totalvocab_tokenized.extend([allwords_tokenized])\n",
    "\n",
    "print(np.shape(totalvocab_tokenized),  np.shape(totalvocab_stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1361 unique tokens: ['sand', 'clay', 'drift', 'suppli', 'water']...)\n",
      "length corpus 130085\n"
     ]
    }
   ],
   "source": [
    "###here just tokens or stemmed tokens can be used alternatively\n",
    "dictionary = gensim.corpora.Dictionary(totalvocab_stemmed)\n",
    "## newt two lines remove tokens contained just once in the dictionary\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "dictionary.filter_tokens(once_ids)\n",
    "\n",
    "dictionary.compactify()\n",
    "print(dictionary)\n",
    "corpus = [dictionary.doc2bow(text) for text in totalvocab_stemmed]\n",
    "print('length corpus', len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw2 = ['redish', 'reddish', 'red', 'black', 'blackish', 'brown', 'brownish',\n",
    "          'blue', 'blueish', 'orange', 'orangeish', 'gray', 'grey', 'grayish',\n",
    "          'greyish', 'white', 'whiteish', 'purple', 'purpleish', 'yellow',\n",
    "          'yellowish', 'green', 'greenish', 'light', 'very', 'pink', 'coloured', 'multicoloured',\n",
    "          'dark', 'color', 'colour', 'hard', 'soft', 'water', 'supply', 'fine', 'coarse',\n",
    "          'medium', 'bearing', 'pipe', 'sticky', 'tough', 'small', 'stiff', \n",
    "          'running', 'streaks', 'nominal', 'bands', 'back', 'slippery', 'loose', \n",
    "          'broken', 'fractured', 'surface']\n",
    "\n",
    "\n",
    "stops= stopa+stopw2  # add the additional stopwords above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,) (1000,)\n"
     ]
    }
   ],
   "source": [
    "Namoi = 'reclasified.csv'\n",
    "Data = pd.read_csv(root+Namoi)\n",
    "DataSampled = Data.sample(1000)\n",
    "DataDes=DataSampled.Description.tolist()\n",
    "DataDes = [str(n) for n in DataDes]\n",
    "\n",
    "\n",
    "# #use extend so it's a big flat list of vocab\n",
    "stemmed = []\n",
    "tokenized = []\n",
    "for i in DataDes:\n",
    "    allwords_stemmed = tokenize_and_stem(i, stops) #for each item in 'synopses', tokenize/stem\n",
    "    stemmed.extend([allwords_stemmed]) #extend the 'totalvocab_stemmed' list\n",
    "\n",
    "    allwords_tokenized = tokenize_only(i, stops)\n",
    "    tokenized.extend([allwords_tokenized])\n",
    "\n",
    "print(np.shape(tokenized),  np.shape(stemmed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 114 ms, sys: 94 Âµs, total: 114 ms\n",
      "Wall time: 113 ms\n"
     ]
    }
   ],
   "source": [
    "%time tfidf = gensim.models.TfidfModel(corpus, normalize = True)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfSim = gensim.similarities.Similarity('/home/ignacio/jugo',corpus_tfidf, len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidfSim = gensim.similarities.Similarity()\n",
    "# tfidfSimMatrix =[]\n",
    "simsss = []\n",
    "for n, i in enumerate(tokenized):\n",
    "    vec_bow = dictionary.doc2bow(i)\n",
    "    vec_tfidf = tfidf[vec_bow]\n",
    "    sims1 = tfidfSim[vec_tfidf]\n",
    "    simsss.append(sims1)\n",
    "#     print(sims1)\n",
    "#     tfidfSimMatrix.append(sims1)\n",
    "# tfidfDisMatrix = 1- np.array(tfidfSimMatrix) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.563422275581 0.651132177261\n"
     ]
    }
   ],
   "source": [
    "DataSampled['sim'] = simsss\n",
    "X = np.vstack(DataSampled.sim)\n",
    "kmeans = cluster.KMeans(n_clusters = 90).fit(X)\n",
    "pred_classes = kmeans.predict(X)\n",
    "\n",
    "print(metrics.adjusted_rand_score(DataSampled.OWN.tolist(), pred_classes.tolist()),\n",
    "      metrics.adjusted_mutual_info_score(DataSampled.OWN.tolist(), pred_classes.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70897165896437198"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.completeness_score(DataSampled.OWN.tolist(), pred_classes.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsiSim = gensim.similarities.Similarity('/home/ignacio/jugo',lsi[corpus_tfidf], len(dictionary))\n",
    "simsss =[]\n",
    "for n in tokenized:\n",
    "    vec_bow = dictionary.doc2bow(n)\n",
    "    vec_lsi = lsi[vec_bow]\n",
    "    sims1 = lsiSim[vec_lsi]\n",
    "    simsss.append(sims1)\n",
    "# lsiDisMatrix = 1- np.array(lsiSimMatrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.LdaModel(tfidf[corpus], id2word=dictionary, num_topics=50, update_every=5, chunksize=10000, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaSim = gensim.similarities.Similarity('/home/ignacio/jugo', lda[corpus_tfidf], len(dictionary))\n",
    "simsss =[]\n",
    "for n in tokenized:\n",
    "    vec_bow = dictionary.doc2bow(n)\n",
    "    vec_lda = lda[vec_bow]\n",
    "    sims1 = ldaSim[vec_lda]\n",
    "    simsss.append(sims1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
